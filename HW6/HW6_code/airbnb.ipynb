{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AirBnB Data\n",
    "\n",
    "In this problem, we will predit the price of an AirBnB listing as a function of its attributes. We get our data from this source: https://www.kaggle.com/c/airbnblala/data#\n",
    "\n",
    "Our first step will be to import all of the modules we need, and then load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Random\n",
    "Random.seed!(13)\n",
    "\n",
    "using CSV\n",
    "using Plots\n",
    "using DataFrames\n",
    "using Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = CSV.read(\"airbnb.csv\")\n",
    "\n",
    "# let's list all available categories, as well as their datatype using the \"eltype\" function.\n",
    "feature_names = names(df)\n",
    "for i in 1:96\n",
    "    println(string(i), \"\\t\", string(feature_names[i]), \"\\t\\t\\t\", string(eltype(df[!, i])))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Train / Test Split\n",
    "\n",
    "\n",
    "The last three features are price; let's say that column 61 is the label to predict, and let's hide 62 and 63.\n",
    "\n",
    "We now will generate a \"data\" collection, a \"target\" collection (column 61), and create a train/test split so that we can empirically test for overfitting.\n",
    "\n",
    "To make the 80 / 20 train test split, we are going to shuffle the data, and then select the first $80\\%$ as the train data, with $20\\%$ held out for validation.\n",
    "\n",
    "Use the code below to create a train / test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[.!(ismissing.(df[!, :price])), :]; # let's only consider the examples for which the price is known\n",
    "df = df[shuffle(1:end), :] # we shuffle the data so that our train/test split will be truly random\n",
    "\n",
    "train_proportion = 0.8\n",
    "n = size(df, 1)\n",
    "println(\"Size of dataset: \", string(n))\n",
    "\n",
    "# Put the first ntrain observations in the DataFrame df into the training set, and the rest into the test set\n",
    "ntrain = convert(Int, round(train_proportion*n))\n",
    "\n",
    "target = df[:, :price]\n",
    "data = df[:, filter(col -> (col != :price && col!= :weekly_price && col!= :monthly_price), feature_names)]\n",
    "\n",
    "# the following variable records the features of examples in the training set\n",
    "train_x = data[\n",
    "# the following variable records the features of examples in the test set\n",
    "test_x = data[\n",
    "# the following variable records the labels of examples in the training set\n",
    "train_y = target[\n",
    "# the following variable records the labels of examples in the test set\n",
    "test_y = target[\n",
    "\n",
    "# let's take a look\n",
    "train_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Real-Valued Data\n",
    "\n",
    "Your first task will be to fit a linear model (With bias!) to the simplest type of data available - the real-valued parameters. Once you fit your model to the train data, you will need to print train and test mean squared error, as well as plot the predicted vs the real price.\n",
    "\n",
    "The numeric data includes:\n",
    "\n",
    "  - host_listings_count\n",
    "  - host_total_listings_count\n",
    "  - accomodates\n",
    "  - bathrooms\n",
    "  - bedrooms\n",
    "  - guests_included\t\t\n",
    "  - extra_people\n",
    "  - minimum_nights\n",
    "  - maximum_nights\n",
    "  - availability_30\n",
    "  - availability_60\n",
    "  - availability_90\n",
    "  - availability_365\n",
    "  - number_of_reviews\n",
    "  - review_scores_rating\n",
    "  - review_scores_accuracy\n",
    "  - review_scores_cleanliness\n",
    "  - review_scores_checkin\n",
    "  - review_scores_communication\n",
    "  - review_scores_location\n",
    "  - review_scores_value\n",
    "\n",
    "It's also important to also include the following numeric data, which are stored as strings:\n",
    "\n",
    "  - beds\n",
    "  - security_deposit\n",
    "  - cleaning_fee\n",
    "  \n",
    "For those, we provide a utility function, which interprets any non-numerical value as a \"0\", which is an accurate interpretation of \"NA\" in this context.\n",
    "\n",
    "The starter code below provides lists of these numeric and numeric-as-string features \n",
    "in addition to some data processing routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"This function converts strings to floating point values.\n",
    "Strings that cannot be represented as a number (like NA) are converted to zeros\"\n",
    "function string_to_float(str)\n",
    "    try\n",
    "        parse(Float64, str)\n",
    "    catch\n",
    "       0.0\n",
    "    end\n",
    "end\n",
    "\n",
    "labels_real = [\n",
    "  :host_listings_count,\n",
    "  :host_total_listings_count,\n",
    "  :accommodates,\n",
    "  :bathrooms,\n",
    "  :bedrooms,\n",
    "  :guests_included,\n",
    "  :extra_people,\n",
    "  :minimum_nights,\n",
    "  :maximum_nights,\n",
    "  :availability_30,\n",
    "  :availability_60,\n",
    "  :availability_90,\n",
    "  :availability_365,\n",
    "  :number_of_reviews,\n",
    "  :review_scores_rating,\n",
    "  :review_scores_accuracy,\n",
    "  :review_scores_cleanliness,\n",
    "  :review_scores_checkin,\n",
    "  :review_scores_communication,\n",
    "  :review_scores_location,\n",
    "  :review_scores_value\n",
    "]\n",
    "\n",
    "labels_string = [\n",
    "    :beds,\n",
    "    :security_deposit,\n",
    "    :cleaning_fee\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the real valued data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the following variable should have as many columns as real variables, and as many rows as examples in the training set      \n",
    "train_vals_real = \n",
    "# the following variable should have as many columns as real variables, and as many rows as examples in the test set      \n",
    "test_vals_real ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's extract the string valued data, convert them to floating point numbers, and convert the result to arrays. Remember that $f.( \\ldots )$ means \"apply f elementwise.\" It's possible to complete this code with one line per expression below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the following variable should have as many columns as string variables, and as many rows as examples in the training set      \n",
    "train_vals_from_string = convert(Matrix, string_to_float.(\n",
    "# the following variable should have as many columns as string variables, and as many rows as examples in the test set      \n",
    "test_vals_from_string = \n",
    "        \n",
    "@assert(eltype(train_vals_from_string) != String)\n",
    "@assert(eltype(test_vals_from_string) != String)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To produce our training features, use the real valued features and the string valued features, together with an offset. You can do this using the function `hcat`, which concatenates matrices (with an equal number of rows) horizontally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain = hcat(\n",
    "Xtest = hcat("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit the linear model, compute and print the MSE, and plot the predicted versus the real data. Some portions below are left for you to fill in.\n",
    "\n",
    "We provide some useful helper functions below, though we ask you to complete the MSE function.\n",
    "\n",
    "Plotting all the test data might significantly slow down this notebook; if that happens, feel free to plot just the first couple hundred points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"This function just computes the mean squared error.\"\"\"\n",
    "function MSE(y, pred)\n",
    "    \"Fill this in.\"\n",
    "end\n",
    "\n",
    "\"\"\"This function plots the main diagonal; \n",
    "for a \"predicted vs true\" plot with perfect predictions,\n",
    "all data lies on this line\"\"\"\n",
    "function plotDiagonal(xmin, xmax)\n",
    "    xsamples = [xmin, xmax]\n",
    "    plot!(xsamples, xsamples, color=:black)\n",
    "end\n",
    "\n",
    "\"\"\"This helper funciton plots x vs, y and labels the axes.\"\"\"\n",
    "function plotdata(x,y,xname, yname; margin=.05, plotDiag=true, zeromin=false)\n",
    "    scatter(x,y, label=\"data\")\n",
    "    xlabel!(xname)\n",
    "    ylabel!(yname)\n",
    "    range_y = maximum(y) - minimum(y)\n",
    "    range_x = maximum(x) - minimum(x)\n",
    "    if plotDiag\n",
    "        plotDiagonal(minimum(x)-margin*range_x, maximum(x)+margin*range_x)\n",
    "    end\n",
    "    if zeromin\n",
    "        ylims!((0.0,maximum(y)+margin*range_y))\n",
    "        xlims!((0.0,maximum(x)+margin*range_x))\n",
    "    else\n",
    "        ylims!((minimum(y)-margin*range_y,maximum(y)+margin*range_y))\n",
    "        xlims!((minimum(x)-margin*range_x,maximum(x)+margin*range_x))\n",
    "    end\n",
    "end\n",
    "\n",
    "\"\"\"This function plots the predicted labels vs the actual labels\n",
    "(We only plots the first 1000 points to avoid slow plots.)\"\"\"\n",
    "function plot_pred_true(test_pred, test_y, max_points = 1000)\n",
    "    plotdata(test_pred[1:max_points], test_y[1:max_points], \"Predicted (\\$)\", \"True (\\$)\", zeromin=true)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The weights of your linear equation (compute via least squares)\n",
    "w = \n",
    "\n",
    "train_pred = \n",
    "test_pred = \n",
    "\n",
    "train_MSE = MSE(\n",
    "test_MSE = MSE("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "println(\"Train MSE\\t\", train_MSE)\n",
    "println(\"Test MSE \\t\", test_MSE)\n",
    "\n",
    "plot_pred_true(test_pred, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions trend in the right direction, but there is still plenty of room for improvement.\n",
    "\n",
    "We are missing some of the most important data: the type of rental. After all, a well-reviewed tent will never cost as much as a poorly-reviewed house.\n",
    "\n",
    "Unfortunately, this data is stored in categorical columns, including:\n",
    "\n",
    "  - property_type\n",
    "  - room_type\n",
    "  - bed_type\n",
    "  - cancellation_policy\n",
    "  - host_response_time\n",
    "  \n",
    "We will transform them with a one-hot (boolean) encoding. \n",
    "\n",
    "There are also many boolean parameters, which might inform the price:\n",
    "\n",
    "  - require_guest_profile_picture\n",
    "  - require_guest_phone_verification\n",
    "  - instant_bookable\n",
    "  - is_business_travel_ready\n",
    "  - has_availability\n",
    "  - is_location_exact\n",
    "  - host_identity_verified\n",
    "  - host_has_profile_pic\n",
    "  - host_is_superhost\n",
    "  \n",
    "One additional category takes set values:\n",
    "\n",
    "  - host_verifications\n",
    "\n",
    "Let's handle the boolean values first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Boolean Data\n",
    "\n",
    "We've provided a helper function to convert the values stored in the original dataset - \"t\" and \"f\" - into Julia \"true\" and \"false.\" Extract the arrays for these parameters, then concatenate these features with the features from the previous part.\n",
    "\n",
    "Note that \"hcat\" will smoothly handle type conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converts from \"t\" and \"f\" into true and false\n",
    "function string_to_bool(str)\n",
    "    str == \"t\"\n",
    "end\n",
    "\n",
    "bool_labels = [\n",
    "  :require_guest_profile_picture,\n",
    "  :require_guest_phone_verification,\n",
    "  :instant_bookable,\n",
    "  :is_business_travel_ready,\n",
    "  :has_availability,\n",
    "  :is_location_exact,\n",
    "  :host_identity_verified,\n",
    "  :host_has_profile_pic,\n",
    "  :host_is_superhost\n",
    "]\n",
    "\n",
    "# the following variable should have as many columns as real variables, and as many rows as examples in the training set      \n",
    "train_bv = \n",
    "# the following variable should have as many columns as real variables, and as many rows as examples in the test set      \n",
    "test_bv = \n",
    "\n",
    "@assert(eltype(train_bv) == Bool)\n",
    "@assert(eltype(test_bv) == Bool)\n",
    "\n",
    "# concatenate the real and boolean features to form your X matrix\n",
    "Xtrain = hcat(\n",
    "Xtest = hcat("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all that's left is to again fit a function and compute the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = \n",
    "\n",
    "train_pred = \n",
    "test_pred = \n",
    "\n",
    "train_MSE = \n",
    "test_MSE = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "println(\"Train MSE\\t\", train_MSE)\n",
    "println(\"Test MSE \\t\", test_MSE)\n",
    "\n",
    "plot_pred_true(test_pred, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That didn't help much! In fact this is not too surprising. The most informative data is categorical for this dataset: a well-reviewed tent will never cost as much as a poorly reviewed full house.\n",
    "\n",
    "## d) Categorical Data\n",
    "\n",
    "Transform the data into one-hot vectors. The categories are easy to extract using the \"unique\" function.\n",
    "\n",
    "Here, we ask that you contribute a little more to the helper function, since this transformation is more challenging. Notice that the categories are passed as an argument - this is important, in case the test set doesn't have a representation for all of the categories in the training set.\n",
    "\n",
    "Make sure your function is linear in the number of data points (entries in the column) which it processes. You can use the nested loop structure provided, or clever list comprehensions. It is possible (but not required) to use a \"Dataframe\" object in a clever way to make the function significantly faster.\n",
    "\n",
    "If a category appears in the test set but not in the training set, the \"one hot\" vector should be a vector of zeros for that parameter.\n",
    "\n",
    "We provide hcat syntax here that makes applying the onehot function to every categorical column at once relatively easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_labels = [\n",
    "  :property_type,\n",
    "  :room_type,\n",
    "  :bed_type,\n",
    "  :cancellation_policy,\n",
    "  :host_response_time\n",
    "]\n",
    "\n",
    "#Sets of all categories in a particular column\n",
    "cats_sets = [unique(train_x[:, label]) for label in cat_labels]\n",
    "\n",
    "\"Computes a onehot vector for every entry in column given a set of categories cats\"\n",
    "function onehot(column, cats=unique(column))\n",
    "    result = zeros( ,  )\n",
    "    for \n",
    "        for \n",
    "            \n",
    "            \n",
    "            \n",
    "        end\n",
    "    end\n",
    "    result\n",
    "end\n",
    "\n",
    "\n",
    "train_cat_vals = hcat([onehot(train_x[:, cat_labels[i]], cats_sets[i]) for i in 1:size(cat_labels, 1)]...)\n",
    "test_cat_vals = hcat([onehot(test_x[:, cat_labels[i]], cats_sets[i]) for i in 1:size(cat_labels, 1)]...) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now include the categorical features along with the rest real, string, and Boolean features and compute the MSE of the resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vals = \n",
    "test_vals = \n",
    "\n",
    "w = \n",
    "\n",
    "train_pred = \n",
    "test_pred = \n",
    "\n",
    "train_MSE = MSE(\n",
    "test_MSE = MSE(\n",
    "        \n",
    "println(\"Train MSE\\t\", train_MSE)\n",
    "println(\"Test MSE \\t\", test_MSE)\n",
    "\n",
    "plot_pred_true(test_pred, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're making significant improvements! We have reduced the MSE from before by $10 \\%$ using this set of parameters.\n",
    "\n",
    "## e) Set data\n",
    "\n",
    "There is another type of data - stored in the host_verifications column - that we haven't used yet. This is set data, best encoded with a many-hot vector.\n",
    "\n",
    "This particular column does not turn out to be particularly informative, but it is valuable to learn how to work with sets.\n",
    "\n",
    "Notice is that the set data happens to be stored in a format that is very close to a valid declaration of a Julia string array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(train_x[1, :host_verifications])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need to do is run \"replace\" to fix the single quotes into double quotes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replace(train_x[1, :host_verifications], \"'\" => \"\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is that an improvement? Well, let's talk about how Julia handles input in general. Every input first appears as a string, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "command = \"2 + 2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command is then parsed into an intermediate representation known as an \"Expr\" type. This is useful for Julia so that it can break apart the epxression into underlying parts later, and evaluate intermediate results as they are needed (we won't go into much detail about interpreted languages here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ex = Meta.parse(command)\n",
    "typeof(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To go from expression to result, the \"eval\" command is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all this in mind, write a simple function which takes one of the strings in the column :host_verifications, and outputs the corresponding String array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_input = \"['email', 'facebook']\"\n",
    "desired_ouput = [\"email\", \"facebook\"]\n",
    "\n",
    "prepro1(s) = \n",
    "\n",
    "# Check that prepro1 consumes sample_input to produce desired_output.\n",
    "# Also check it on the sample below:\n",
    "\n",
    "a = prepro1(train_x[1, :host_verifications])\n",
    "println(a)\n",
    "println(typeof(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important: Be very careful using this technique in practice! \n",
    "Directly parsing and executing data that comes in String form opens a fantastic door to hackers, especially if you are gathering data in an online way for any institution that handles money. If you aren't careful, what I described is a fantastic way to get SQL-injected. In this case, however, we have full knowledge of the data and its source, and it simply makes more sense to use Julia's built-in parsing functions than write our own from scratch (not that this would be a true hurdle). Usually, before handling data in this way, we would need to do substantially more sanitization. It's good to see a little under-the-hood and understand the tools that you are using - similar functions are avaialable in Python and other interpreted languages - but it's also very important to know the vulnerabilities of your code.\n",
    "\n",
    "\n",
    "\n",
    "We can now apply this function elementwise, to the entire column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verif_tr = prepro1.(train_x[:, :host_verifications])\n",
    "verif_te = prepro1.(test_x[:, :host_verifications])\n",
    "\n",
    "verif_tr[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, if you are working on your project, there is a decent chance that sets were stored in a format similar to what we saw here. Using similar string compositions, you will likely be able to similarly parse the data into Julia container types (whether arrays or proper Julia sets)\n",
    "\n",
    "Now, take this array of arrays, and turn it into a many-hot vector. You are welcome to use the nested loop structure below, or array comprehensions, or even the DataFrame object if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the collection of all types we need to worry about.\n",
    "# Notice the use of \"vcat,\" which is like \"hcat\" but vertical\n",
    "vtypes = unique(vcat(verif_tr...))\n",
    "\n",
    "\n",
    "\"Computes a manyhot vector for every entry in an array of arrays given a set of categories cats\"\n",
    "function manyhot(column, cats=unique(col))\n",
    "    result = zeros( , )\n",
    "    for \n",
    "        sete_of_descriptions = \n",
    "        for description in set_of_descriptions\n",
    "            for \n",
    "                \n",
    "                \n",
    "                \n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    result\n",
    "end\n",
    "\n",
    "\n",
    "vtr_vals = manyhot(verif_tr, vtypes)\n",
    "vte_vals = manyhot(verif_te, vtypes)\n",
    "\n",
    "print(vtypes)\n",
    "vtr_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will attach it to the full list of values used so far, but it won't change the MSE much, unfortunately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vals = hcat(\n",
    "test_vals = hcat(\n",
    "\n",
    "w = \n",
    "\n",
    "train_pred = \n",
    "test_pred = \n",
    "\n",
    "train_MSE = MSE(\n",
    "test_MSE = MSE(\n",
    "        \n",
    "println(\"Train MSE\\t\", train_MSE)\n",
    "println(\"Test MSE \\t\", test_MSE)\n",
    "\n",
    "plot_pred_true(test_pred, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f) Location, location, location\n",
    "\n",
    "If you remember, a common refrain in Real Estate is \"location, location, location.\" We're treating Manhattan properties with the same formula as Staten Island!\n",
    "\n",
    "The most naive way to include location data would be with the categories \"latitude\" and \"longitude\" - after all, these are continuous variables, so why not include them in the simplest manner possible?\n",
    "\n",
    "Load the longitude and lattitude data in, then fit a model to predict the labels using *only* these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loc_labels = [\n",
    "    :latitude,\n",
    "    :longitude\n",
    "]\n",
    "\n",
    "# Extract these as real values, append a bias (all ones) feature, \n",
    "# fit a least squares model, and compute the MSE.\n",
    "\n",
    "train_MSE = MSE(\n",
    "test_MSE = MSE(\n",
    "        \n",
    "println(\"Train MSE\\t\", train_MSE)\n",
    "println(\"Test MSE \\t\", test_MSE)\n",
    "\n",
    "plot_pred_true(test_pred, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, this predictor is insufficient. Price is a nonlinear function of location; simply going north or south does not linearly affect the rental price.\n",
    "\n",
    "There are several solutions. We could allow higher order polynomials in longitude and latitude:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create arrays of polynomial features in latitude and longitude up to order 2\n",
    "train_lv2 = hcat(\n",
    "    train_loc_vals[:, 1], train_loc_vals[:, 2],\n",
    "    train_loc_vals[:, 1].^2, train_loc_vals[:, 1].*train_loc_vals[:, 2], train_loc_vals[:, 2].^2,\n",
    "    ones(size(train_loc_vals, 1))\n",
    ")\n",
    "    \n",
    "test_lv2 = hcat(\n",
    "    test_loc_vals[:, 1], test_loc_vals[:, 2],\n",
    "    test_loc_vals[:, 1].^2, test_loc_vals[:, 1].*test_loc_vals[:, 2], test_loc_vals[:, 2].^2,\n",
    "    ones(size(test_loc_vals, 1))\n",
    ")\n",
    "\n",
    "# build a model using these polynomial features of latitude and longitude\n",
    "w_loc = \n",
    "\n",
    "train_pred = \n",
    "test_pred = \n",
    "\n",
    "train_MSE = MSE(\n",
    "test_MSE = MSE(\n",
    "        \n",
    "println(\"Train MSE\\t\", train_MSE)\n",
    "println(\"Test MSE \\t\", test_MSE)\n",
    "\n",
    "plot_pred_true(test_pred, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polynomial expansion is not particularly useful (or sensible) for location. Instead, let's consider a simpler feature transformation. \n",
    "\n",
    "Every location can be associated not only with its longitude and latitude, but also with a number of other categorical location parameters. Ones that appear in this dataset include:\n",
    "\n",
    "  - street\n",
    "  - neighbourhood\n",
    "  - neighbourhood_cleansed\n",
    "  - neighbourhood_group_cleansed\n",
    "  - city\n",
    "  - state\n",
    "  - zipcode\n",
    "  - market\n",
    "  - smart_location\n",
    "  - country_code\n",
    "  - country\n",
    "  \n",
    "Let's see if zipcode predicts price well.\n",
    "\n",
    "In the next cell, build a model that predicts price using a one-hot encoding of zipcode. Print the train and test MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad for one (categorical) feature! Now fit a model on both the one-hot zip code together with the other features that we've already accumulated from the previous parts. Print the MSE scores and draw the plots for the full set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is by far the best result so far. It seems that location brought in some new information, giving us significantly more predictive power.\n",
    "\n",
    "Can we do better by using text data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g) Text\n",
    "\n",
    "The following columns consist of long-form textual descriptions:\n",
    "\n",
    "  - name\n",
    "  - summary\n",
    "  - space\n",
    "  - neighborhood_overview\n",
    "  - notes\n",
    "  - transit\n",
    "  - access\n",
    "  - interaction\n",
    "  - house_rules\n",
    "\n",
    "We will experiment with using features from a pretrained neural network to represent this data.\n",
    "The neural network has learned an internal representation (say, at some hidden layer of the network) that it uses to make its predictions on a different task (such as sentiment prediction for sentences). \n",
    "\n",
    "We will use this internal representation directly for our task of AirBnB price prediction.\n",
    "Using an internal representation from an unrelated model as a feature is a common technique known as transfer learning.\n",
    "\n",
    "We will use pretrained features from the Universal Sentence Encoder (USE), which was designed explicitly for transfer learning to different tasks. \n",
    "[This paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46808.pdf) documents how this neural network architecture was designed and trained: \n",
    "This network consumes text of arbitrary length, and produces a feature vector of length $512$. \n",
    "\n",
    "Your TAs have precomputed these features for all of the columns above into the table \"airbnb-use-embeddings.csv\",\n",
    "available [as a zip file on the course website](https://people.orie.cornell.edu/mru8/orie4741/homework/airbnb-use-embeddings.zip).\n",
    "The id of each AirBnB listing is provided, along with features with names of the form \"column_number:feature_number\". That is, the 468th feature of the 5th text data column (also known as the \"name\" column) will be found at '5:468'.\n",
    "\n",
    "You can use [this demo code](https://tfhub.dev/google/universal-sentence-encoder/2)) to explore the USE embedding further. If you would like to use this (or any other) embedding for your project, you are welcome to come to Office Hours and we will gladly help you with your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to the AirBnB dataset, we can import the data into a new dataframe. Since our training data has been shuffled, we use a dataset join to assemble the data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tf = CSV.read(\"airbnb-use-embeddings.csv\")\n",
    "\n",
    "train_all = join(train_x, df_tf, on=:id, kind=:left)\n",
    "test_all = join(test_x, df_tf, on=:id, kind=:left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(size(train_x))\n",
    "train_embed = convert(Matrix, train_all[:, 94:end])\n",
    "test_embed = convert(Matrix, test_all[:, 94:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out how useful these parameters are (or aren't), fit a linear model of price as a function of embedding only (with offset!). Keep in mind that the backslash operator may require several minutes of computation for such a large problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two things are readily apparent: this embedding provides a lot of information, but we are massively overfitting. This is to be expected: after all we have $4689$ parameters to fit, but only $20000$ training points.\n",
    "\n",
    "Now, is this embedding is capturing new information compared to the other columns? \n",
    "To test this, fit a least squares predictor using all of the features we have discussed in this notebook.\n",
    "Compute MSE and plot the predicted vs expected score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit\n",
    "\n",
    "How can you explain these results? And is there any way you can fix them to improve the test error?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
